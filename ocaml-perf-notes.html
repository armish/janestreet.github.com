<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print" />

    <script type="text/javascript" src="shjs/sh_main.min.js"></script>
    <script type="text/javascript" src="shjs/sh_caml.min.js"></script>
    <script type="text/javascript" src="shjs/sh_c.min.js"></script>
    <link rel="stylesheet" type="text/css" href="shjs/sh_navy.css"/>

    <title>Open Source @ Jane Street</title>
  </head>

  <body onload="sh_highlightDocument();">

    <header>
      <div class="container">
        <h1>Open Source @ Jane Street</h1>

        <section id="downloads">
          <a href="https://github.com/janestreet">View on GitHub</a>
          <a href="https://bitbucket.org/janestreet"> View on Bitbucket </a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">

<h1 id="writing-performance-sensitive-ocaml-code">Writing Performance Sensitive OCaml Code</h1>
<h2 id="ocaml-performance">1) OCaml Performance</h2>
<p>The notes fall into the following categories:</p>
<ul>
<li>notes about the cost of commonly used OCaml constructs</li>
<li>general recommendations to help speed up OCaml code</li>
<li>notes on reading OCaml assembly code so that you may understand what your own OCaml programs do on the CPU.</li>
</ul>
<p>When possible have tried to motivate these with snippets of assembly code generated by ocamlopt.</p>
<p>As a caveat, performance optimizations are sometimes at odds with desirable programming styles such as preserving abstraction boundaries and higher order functional patterns. Hand inlining code across abstraction boundaries and avoiding closure allocations buy you some performance, however these also make code brittle and inflexible. One should do these sorts of optimizations sparingly and only in the heart of tight loops.</p>
<p>Possibly the most useful performance tuning tip is to identify the places where your program spends most time and see if there are high level algorithmic approaches and changes in representation of data that can speed up things. These high level approaches will usually buy you more than machine level optimizations. Resort to low level tuning described in this document only after you have tried the former.</p>
<h3 id="compiling">1.1) Compiling</h3>
<p>Most of the discussion below assumes a 64bit platform. Assembly code presented here is x86_64 asm generated by using ocamlopt with the compiler flags <code>-S</code>, <code>-inline 20</code> and <code>-nodynlink</code> on a 64bit Linux platform.</p>
<p>You can add the <code>-S</code> flag, which instructs the compiler to generate assembly code, by adding</p>
<pre class="sh_caml">OCAMLOPTFLAGS += -S</pre>
<p>to your local <code>OMakefile</code>.</p>
<p>It is best to generate assembly code with full inlining and other optimizations that the compiler supports. Even thought inlining and optimization might make the code a bit harder to read, it will give you a more accurate picture of what executes on the CPU. Profiling tool like gprof where the inlining and such only serves to confuse the profiler.</p>
<h2 id="reading-ocaml-assembly">2) Reading OCaml Assembly</h2>
<p>This section introduces the minimal assembly instruction patterns that one must know to follow some of the cost arguments later. This is not a general introduction to x86 assembly.</p>
<h3 id="named-functions-are-mangled-but-recognizable.">2.1) Named functions are mangled but recognizable.</h3>
<p>In the file called test_asm.ml the function f</p>
<pre class="sh_caml">let f a = a+1</pre>
<p>The generated asm looks like the following:</p>
<pre class="sh_caml">camlTest_asm__f_33700:
.L119:
        addq    $2, %rax
        ret</pre>
<p>Note that the file/module name and the name of the function appear in the generated label. An anonymous function however will just have some numeric label making it difficult to locate.</p>
<h3 id="memory-allocation">2.2) Memory allocation</h3>
<p>When OCaml allocates memory it decrements the &quot;heap top&quot; which is typically in %r15 followed by an overflow check. If there is an overflow it calls the GC and the allocation is retried.</p>
<p>The snippet below allocates 24 bytes (three 64-bit words).</p>
<pre class="sh_caml">.L121:  subq    $24, %r15
        cmpq    caml_young_limit(%rip), %r15
        jb      .LWoohoo! You&#39;ve read all the important messages in your inbox122</pre>
<p>This is the code at label .L122 that has the call to the GC followed by the jump to retry allocation with the new %r15 and young limit value after garbage collection.</p>
<pre class="sh_caml">.L122:  call    caml_call_gc
.L123:  jmp     .L121</pre>
<h3 id="boxed-floats-and-immediate-ints">2.3) Boxed Floats and Immediate ints</h3>
<p>OCaml uses IEEE 64-bit floats and are usually heap allocated (ie. boxed). The main reason for this is that OCaml needs to tag its values such that the GC can identify them at runtime. OCaml allocates floats on the heap using two words - the first word is a tag that says that the next is a float.</p>
<p>In asm the following snippet indicates a typical boxing operation where the actual float value is in the register %xmm0. Looking out for these when trying to track down boxed floats in your code.</p>
<ol style="list-style-type: decimal">
<li><p>First allocate the 2 words.</p>
<pre class="sh_caml">.L120:  subq    $16, %r15
        cmpq    caml_young_limit(%rip), %r15
        jb      .L121</pre></li>
<li><p>Assign the tag value (the first word).</p>
<pre class="sh_caml">       leaq    8(%r15), %rax
       movq    $1277, -8(%rax)</pre></li>
<li><p>Assign the actual value (the second word).</p>
<pre class="sh_caml">       movlpd  %xmm0, (%rax)-</pre></li>
</ol>
<p>NOTE: An important thing to note is that a pointer to a heap allocated value does not point to the tag word but the actual first value. This is why the &quot;load-effective-address&quot; (lea) instruction adds the offset of 8 bytes. This is true for all heap allocated values, not just floats.</p>
<p>Unlike floats, ints are represented as immediates, i.e., they are not allocated on the heap. To tag ints OCaml steals the LSB bit of an int and sets it to 1 always. This means that we get 63 bits of precision in OCaml ints and any int value n will appear in the assembly as 2*n+1 (the LSB is always 1 and the bits of the number shifted one place in).</p>
<h3 id="tuples">2.4) Tuples</h3>
<p>Tuples are heap allocated. A pair will cause the allocation of 3 words: one word to indicate that it is a pair and two words for the actual values.</p>
<p>If a tuple contains floats, the floats will be boxed. If one wishes to return a pair of floats, a record containing the two floats may be more efficient than using a tuple - more on this later.</p>
<h3 id="option-types">2.5) Option types</h3>
<p>Option types are heap allocated. Passing a value as an option will cause memory allocation. The value None is however immediate and represented as 1, the same as the representation of the int value 0.</p>
<h3 id="write-barriers">2.6) Write barriers</h3>
<p>A write barrier is a basically a call to caml_modify and looks like the following:</p>
<pre class="sh_caml">call    caml_modify</pre>
<p><a href="http://web.archive.org/web/20111114164033/http://eigenclass.org/R2/writings/optimizing-caml_modify">This webpage</a> and <a href="http://web.archive.org/web/20100816130113/http://eigenclass.org/R2/writings/write-barrier-cost">this one</a> give a reasonable overview of what <code>caml_modify</code> does. Any introductory text on garbage collection should give you some insight into why write barriers exist.</p>
<h3 id="asserts-are-helpful-in-reading-asm.">2.7) Asserts are helpful in reading asm.</h3>
<p>If you have a large function and wish to see how a particular part of the function translates into asm, a simple way to track down the fragment in assembly is to put asserts around it. This is just a hack, but in practice it saves you some time reading asm.</p>
<p>Sometimes OCaml tends to optimize away provably correct asserts - this is something to watch out for. In the generated asm, the presence of the assert is indicated by:</p>
<pre class="sh_caml">leaq    caml_exn_Assert_failure(%rip), %rbx</pre>
<p>Hence if you are looking for a snippet of code in function f, first find f and then look for the asserts that you put in it.</p>
<h2 id="cost-of-ocaml-constructs">3) Cost of OCaml constructs</h2>
<h3 id="cost-of-memory-allocation">3.1) Cost of memory allocation</h3>
<p>Having seen the OCaml memory allocation instructions its easy to see that memory allocation itself is cheap - its just one addition and a test, in the common case.</p>
<p>The real cost of memory allocation comes during GC time. The more memory you allocate, even though the allocations are cheap, the more time your program will spend in the GC. So a general rule of thumb for speeding up programs is to reduce allocation rates. Useful reading includes:</p>
<ul>
<li><a href="http://www.ocaml-tutorial.org/garbage_collection"><code class="url">http://www.ocaml-tutorial.org/garbage_collection</code></a></li>
<li><a href="http://rwmj.wordpress.com/2009/08/06/ocaml-internals-part-3-the-minor-heap/"><code class="url">http://rwmj.wordpress.com/2009/08/06/ocaml-internals-part-3-the-minor-heap/</code></a></li>
<li><a href="http://rwmj.wordpress.com/2009/08/06/ocaml-internals-part-3-the-minor-heap/"><code class="url">http://rwmj.wordpress.com/2009/08/06/ocaml-internals-part-3-the-minor-heap/</code></a></li>
<li><a href="http://rwmj.wordpress.com/2009/08/07/ocaml-internals-part-4-the-major-heap/"><code class="url">http://rwmj.wordpress.com/2009/08/07/ocaml-internals-part-4-the-major-heap/</code></a></li>
<li><a href="http://rwmj.wordpress.com/2009/08/07/ocaml-internals-part-4-the-major-heap/"><code class="url">http://rwmj.wordpress.com/2009/08/07/ocaml-internals-part-4-the-major-heap/</code></a></li>
</ul>
<h3 id="closure-allocation">3.2) Closure allocation</h3>
<p>One significant source of memory allocation is the allocation of closures. Closure allocation basically involves allocating memory for each of the free variables of the function and for a code (memory operations are more expensive than register operations) and pointer.</p>
<ol style="list-style-type: decimal">
<li><p>If a function may be called just once or not called at all, try and restructure your code to not allocate the closure. For example: if one is going to fold or map over a list that is frequently empty, testing to see if the list is empty before doing the map operation will avoid the closure allocation.</p></li>
<li><p>Alternately, try to avoid writing functions that have free variables - instead pass all the arguments explicitly.</p></li>
</ol>
<h3 id="inlining">3.3) Inlining</h3>
<p>It is easier to describe when the compiler does not inline rather than when it does.</p>
<ol style="list-style-type: decimal">
<li>It does not inline large functions. I don't know the exact metric.</li>
<li>It does not inline &quot;rec&quot; bound functions - even if they are small and even if the function is not really recursive.</li>
<li>It does not inline functions that are available through functor application even if they are determinable statically.</li>
</ol>
<p>In other cases you should look at the assembly to see if the function has been inlined or not.</p>
<h3 id="stack-allocation-of-arguments">3.4) Stack allocation of arguments</h3>
<p>Only a small number of arguments are passed in registers to functions - the actual number varies according to platform and possibly other optimizations that the compiler performs.</p>
<p>The remainder of the arguments have to passed via the stack. Hence functions that receive a large number of arguments will involve several memory operations for processing the callee saved register and then the stack allocation of formal arguments. In cases where it is feasible, breaking up the called function into smaller functions will allow for the arguments to be passed in registers and in possibly allow for inlining of the resulting smaller functions.</p>
<h3 id="polymorphic-compare">3.5) Polymorphic compare</h3>
<p>OCaml provides a polymorphic compare <code>caml_compare</code> for datatypes. It works in much the same way the GC does by looking at the tags on objects to determine their types at runtime and then using the appropriate compare functions. This is more expensive than using a compare operation written specifically for the actual type (for example <code>caml_float_compare</code> for comparing floats).</p>
<pre class="sh_caml">let cmp a b =
  compare a b</pre>
<p>in assembly:</p>
<pre class="sh_caml">camlTest_asm__cmp_33386:
        subq    $8, %rsp
        movq    %rax, %rdi
        movq    %rbx, %rsi
        leaq    caml_compare(%rip), %rax
        call    caml_c_call</pre>
<p>Here is an explicit float comparison:</p>
<pre class="sh_caml">let cmp_float1 (a : float) (b : float) =
  Float.compare a b</pre>
<p>in assembly:</p>
<pre class="sh_caml">camlTest_asm__cmp_float1_33390:
        subq    $8, %rsp
        movq    %rax, %rdi
        movq    %rbx, %rsi
        call    caml_float_compare
        addq    $8, %rsp
        ret</pre>
<p>Here is an inferred float comparison:</p>
<pre class="sh_caml">let cmp_float3 (a : float) (b : float) =
  compare a b</pre>
<p>in assembly:</p>
<pre class="sh_caml">camlTest_asm__cmp_float3_33679:
        subq    $8, %rsp
.L107:
        movq    %rax, %rdi
        movq    %rbx, %rsi
        call    caml_float_compare
        addq    $8, %rsp
        ret</pre>
<h3 id="inlining-and-polymorphic-compare">3.6) Inlining and polymorphic compare</h3>
<p>In the following snippets the compiler can infer that the compare is only float comparison, however the inliner does not specialize the comparison operation.</p>
<pre class="sh_caml">let cmp_float2 (a : float) (b : float) =
  cmp a b</pre>
<p>Note that the inlining has happened as expected, but no specialization:</p>
<pre class="sh_caml">camlTest_asm__cmp_float2_33676:
        subq    $8, %rsp
        movq    %rax, %rdi
        movq    %rbx, %rsi
        leaq    caml_compare(%rip), %rax
        call    caml_c_call
        addq    $8, %rsp
        ret</pre>
<p>Doing just renaming also does not buy us anything, it is just as bad.</p>
<pre class="sh_caml">let cmp_compare = compare

let cmp_float4 (a : float) (b : float) =
  cmp_compare a b</pre>
<p>As before, inlined but not specialized.</p>
<pre class="sh_caml">camlTest_asm__cmp_float4_33682:
        subq    $8, %rsp
        movq    %rax, %rdi
        movq    %rbx, %rsi
        leaq    caml_compare(%rip), %rax
        call    caml_c_call
        addq    $8, %rsp
        ret</pre>
<p>So when in doubt, manually specify the type specific compare function.</p>
<h3 id="float-boxing.-time-is-usually-represented-as-a-float.">3.7) Float boxing. Time is usually represented as a float.</h3>
<p>Why are floats boxed/unboxed? GC, IEEE format, SSE instructions, tagging... this is a long story and one should look for descriptions of this online. Here are some general rules about when floats occur boxed and unboxed:</p>
<ol style="list-style-type: decimal">
<li>Float arguments to (non-inlined) functions are boxed.</li>
<li>Float return values from (non-inlined) functions are boxed.</li>
<li>Floats have to be unboxed for doing calculations - they usually use the SSE instructions of the CPU.</li>
<li>An array of floats uses an immediate representation - ie. each float is not boxed.</li>
<li>A record of consisting only of floats uses an immediate representation. This does not apply to polymorphic records that get specialized to floats. This does not apply to abstract types that are internally defined to be floats.</li>
<li>An record that has floats and non-floats has the floats boxed.</li>
<li>A tuple of floats has the individual floats boxed.</li>
</ol>
<h3 id="float-compare">3.8) Float compare</h3>
<p>Compare for floats caml_float_compare requires the floats to be boxed. This can be terrible when you have immediate floats.</p>
<pre class="sh_caml">type t = {
  x : float;
  y : float;
}
let cmp_t1 a b =
  begin match compare a.x b.x with
  0 -&gt; compare a.y b.y
  x -&gt; x
  end</pre>
<p>Becomes the following. Notice the boxing operations for the floats.</p>
<pre class="sh_caml">camlTest_asm__cmp_t1_33693:
        subq    $8, %rsp
.L113:
        movq    %rax, %rbp
.L114:  subq    $32, %r15
        cmpq    caml_young_limit(%rip), %r15
        jb      .L115
        leaq    8(%r15), %rsi
        movq    $1277, -8(%rsi)
        movlpd  (%rbx), %xmm0
        movlpd  %xmm0, (%rsi)
        leaq    16(%rsi), %rdi
        movq    $1277, -8(%rdi)
        movlpd  (%rbp), %xmm0
        movlpd  %xmm0, (%rdi)
        call    caml_float_compare
        cmpq    $1, %rax
        je      .L112
        addq    $8, %rsp
        ret
        .align  4
.L112:
.L117:  subq    $32, %r15
        cmpq    caml_young_limit(%rip), %r15
        jb      .L118
        leaq    8(%r15), %rsi
        movq    $1277, -8(%rsi)
        movlpd  8(%rbx), %xmm0
        movlpd  %xmm0, (%rsi)
        leaq    16(%rsi), %rdi
        movq    $1277, -8(%rdi)
        movlpd  8(%rbp), %xmm0
        movlpd  %xmm0, (%rdi)
        call    caml_float_compare
        addq    $8, %rsp
        ret</pre>
<p>The floats are boxed and then there are calls to the underlying float comparison. A much more efficient, though tedious function to write would be:</p>
<pre class="sh_caml">let cmp_t2 a b =
  if a.x = b.x then begin
    if a.y = b.y then 1
    else if a.y &lt; b.y then -1
    else 1
  end
  else begin
    if a.x &lt; b.x
    then -1
    else 1
  end</pre>
<p>Which becomes:</p>
<pre class="sh_caml">camlTest_asm__cmp_t2_33697:
.L124:
        movlpd  (%rbx), %xmm1
        movlpd  (%rax), %xmm0
        ucomisd %xmm1, %xmm0
        jp      .L121
        jne     .L121
        movlpd  8(%rbx), %xmm1
        movlpd  8(%rax), %xmm0
        ucomisd %xmm1, %xmm0
        jp      .L123
        jne     .L123
        movq    $3, %rax
        ret</pre>
<p>Hence use =, &lt;= and &gt;= when possible in the case of floats. These map directly to the underlying asm instructions. NOTE: It may be useful to rewrite Float.compare using =, &lt;= and &gt;=.</p>
<h3 id="mutable-fields-in-records">3.9) Mutable fields in records</h3>
<p>Mutations have an additional cost when the mutated value is heap allocated - ie. when the value is a boxed float, a list, a tuple, a record etc.</p>
<p>These mutations involve calls to the caml write barrier. A simple benchmark shows that invoking the write barrier repeatedly makes thing about 5-6 times slower. Consider the following snippet: Consider:</p>
<pre class="sh_caml">type t = {
  x : int;
  mutable y : float;
}
let set_y t y =
  t.y &lt;- y</pre>
<p>Which generates the following. Here there are calls the write barrier.</p>
<pre class="sh_caml">camlTest_asm__set_y_33718:
        subq    $8, %rsp
.L136:
        addq    $8, %rax
        movq    %rax, %rdi
        movq    %rbx, %rsi
        call    caml_modify
        movq    $1, %rax
        addq    $8, %rsp
        ret</pre>
<p>Contrast the above with the following. Here the only difference is the type of x which causes the float to have an immediate representation (remember that in mixed records floats are boxed, whereas they are not in float-only records).</p>
<pre class="sh_caml">type t = {
  x : float;
  mutable y : float;
}
let set_y t y =
  t.y &lt;- y</pre>
<p>Which generates:</p>
<pre class="sh_caml">camlTest_asm__set_y_33727:
.L137:
        movlpd  (%rbx), %xmm0
        movlpd  %xmm0, 8(%rax)
        movq    $1, %rax
        ret</pre>
<p>Changing representation of your data to avoid calls to the write barrier will help performance. As a side effect, your code may have the additional overhead of introducing unboxing and boxing instructions and so the next result may or may not be advantageous - one has to profile to sure.</p>
<h3 id="mutable-single-field-float-records-can-outperform-float-refs.">3.10) Mutable single-field float records can outperform float refs.</h3>
<p>As a direct consequence of the above, float ref represents the float as a boxed value. Mutating the ref causes calls to the write barrier. The following type does not:</p>
<pre class="sh_caml">type float_ref = {
  mutable x : float;
}</pre>
<p>While it is tempting to try and represent floats in an immediate way as much as possible, however this sometimes hinder performance rather than help it. The fact that function arguments/returns are boxed can cause repeated boxing and unboxing as the floats are passed around.</p>
<p>Note: A useful experiment to try is to use the above <code>float_ref</code> type everywhere that one uses floats and explicitly cast to float only when one needs to do calculations. With respect to memory layout, <code>float_ref</code> is essentially the same as a boxed float but we explicitly have to insert the boxing and unboxing operations - making these explicit can help us optimize away needless cases and save on mutations.</p>
<h3 id="inlining-avoids-boxing-sometimes.">3.11) Inlining avoids boxing (sometimes).</h3>
<p>If the following function is inlined the argument and return value are not boxed.</p>
<pre class="sh_caml">let f x = x + 0.1</pre>
<p>However, in the following function it is possible that <code>x</code> is used as a boxed float sometimes, the OCaml compiler will always box the float on entry to the function.</p>
<pre class="sh_caml">let f x =
  if rarely_true_flag
  then ls := (x :: !ls);
  x + 0.1</pre>
<p>NOTE: This could be something that can be fixed in the compiler by pushing the box allocation into the conditional.</p>
<h3 id="local-module-definitions-are-not-free">3.12) Local module definitions are not free</h3>
<p>Local module definitions have a runtime cost - they are not purely syntactic.</p>
<pre class="sh_caml">let mod_test x =
  let module M = T1.T2 in
  M.f x</pre>
<p>Generates:</p>
<pre class="sh_caml">camlTest_asm__mod_test_33742:
        movq    camlTest_asm + 120(%rip), %rbx
        movq    8(%rbx), %rbx
        movq    (%rax), %rax
        ret</pre>
<p>Notice that the value is %rbx has no real purpose. The inlined call to M.f is only the instruction movq (%rax), %rax. If we write:</p>
<pre class="sh_caml">module M = T1.T2
let mod_test x =
  M.f x</pre>
<p>We have:</p>
<pre class="sh_caml">camlTest_asm__mod_test_33745:
        movq    (%rax), %rax
        ret</pre>
<h3 id="functor-application-is-a-runtime-operation.">3.13) Functor application is a runtime operation.</h3>
<pre class="sh_caml">module S(T : T_intf) = struct
  let g x = (T.f x) + 1
end
let mod_test2 x =
  let module M = S(T1.T2) in
  M.g x</pre>
<p>Generates:</p>
<pre class="sh_caml">call  camlTest_asm__S_33743</pre>
<p>The name of the functor S shows up as a function name. There is not much one can do about this other than to move functor applications outside function definitions such that they are not repeatedly applied.</p>
<h3 id="use-an-immutable-accumulator-if-possible.">3.14) Use an immutable accumulator if possible.</h3>
<p>Code like the following can be seen from time to time. The first snippet is worse than the later, because the later avoids calls to the write barrier.</p>
<pre class="sh_caml">let ls = ref [] in
if test1
then ls := 1 :: !ls;
if test2
then ls := 2 :: !ls;
if test3
then ls := 3 :: !ls;</pre>
<p>without assignments:</p>
<pre class="sh_caml">let ls = [] in
let ls =
  if test1
  then 1 :: ls
  else ls
in
let ls =
  if test2
  then 2 :: ls
  else ls
in
let ls =
  if test3
  then 3 :: ls
  else ls</pre>
<h3 id="use-array.init-sparingly-when-creating-float-arrays">3.15) Use Array.init sparingly when creating float arrays</h3>
<ol style="list-style-type: decimal">
<li>It allocates a closure.</li>
<li>The returned floats are boxed and they have to unboxed when stored in the array. A better way to create float arrays if to use Array.create and a for loop.</li>
</ol>
<h3 id="tuples-1">3.16) Tuples</h3>
<p>The OCaml compiler allocates and extends the lifetime of many tuples that could be avoided.</p>
<pre class="sh_caml">let x, y = if test
           then 1, 2
           else 3, 4</pre>
<p>causes a tuple allocation that is avoidable - only the variables x and y can be used in the latter code and hence the compiler can avoid the memory allocation under the hood. Similarly:</p>
<pre class="sh_caml">match x, y with
0, 0 -&gt; ..
_, _ -&gt; ...</pre>
<p>causes a tuple allocation that is avoidable.</p>
<p>The following is a memory leak:</p>
<pre class="sh_caml">let (x, y_huge) = e in
(fun .. -&gt; ... x ...)</pre>
<p>Here the compiler translates this to the following thereby not allowing 'y_huge&quot; to be deallocated.</p>
<pre class="sh_caml">let t = e in
(fun .. -&gt; ... (fst t) ...)</pre>
<p>These are fixable in the compiler and for now it is better to just use tuples instead of trying to work around these.</p>
<h3 id="option.iter-is-sometimes-not-inlined">3.17) Option.iter is sometimes not inlined</h3>
<pre class="sh_caml">let opt_iter opt =
  Option.iter opt ~f:(fun x -&gt; x_ref := x)</pre>
<p>Becomes:</p>
<pre class="sh_c">camlTest_asm__opt_iter_33732:
.L140:
        leaq    camlTest_asm__18(%rip), %rbx
        cmpq    $1, %rax
        je      .L139
        movq    (%rax), %rax
        movq    (%rbx), %rdi
        jmp     *%rdi
        .align  4
.L139:
        movq    $1, %rax
        ret</pre>
<p>The compiler does not always do this. I don't know why - one should check.</p>
<p>NOTE: The above code is not as optimal as it could be - in particular the anonymous function itself is not inlined. Even if the inlining is not done it could still be better by moving the functional address lookup inside the conditional and avoiding the assignment to %rax. We should ask the compiler folk to fix this.</p>
<p>One should hand-inline the above code when perf matters.</p>
<pre class="sh_caml">let opt_iter opt =
  match opt with
  Some x -&gt; x_ref := x
  None -&gt; ()</pre>
<h2 id="misc">4) Misc</h2>
<p>Some misc perf notes.</p>
<h3 id="glibc-pow-and-the-ocaml-operator.">1) glibc <code>pow()</code> and the OCaml <code>**</code> operator.</h3>
<p>OCaml's <code>**</code> operator calls <code>pow()</code> in glibc. Until glibc version 2.12, the <code>pow()</code> function from the standard math lib has strange non-linearities in the time it takes to compute <code>pow()</code> depending on the numeric values of arguments.</p>
<p>Consider:</p>
<pre class="sh_caml">x = 0.000000946953708742651062124889 beta_s = 0.499999900000000
likelihood  Time: 745181727 ticks 242.797ms

x = 0.000000946953708742651062124889 beta_s = 0.500000000000000
likelihood  Time: 5211833102 ticks 1.55941s

x = 0.000000946953708742651062124889 beta_s = 0.500000010000000
likelihood  Time: 809538470 ticks 261.576ms</pre>
<p>In the above 3 cases, the <code>beta_s</code> value is used as the exponential value and it is interesting to note that when <code>beta_s</code> is exactly 0.5 the computation takes much longer. The snippet of code in glibc that determines when to take the slow is somewhat impenetrable and so at this point we are unsure which other numbers are slow. 1.5, 2.5, 0.41666... are some that seem to be slow.</p>
<p>The current workaround for the problem is to reduce the precision of x in pow(x, y), by doing:</p>
<pre class="sh_caml">(Float.round_nearest (x *. 1e8)) *. 1e-8.</pre>
<p>This behavior is apparently fixed in glibc 2.14. There is surprisingly little about this issue on the Internet. Here are two somewhat relevant links:</p>
<p><a href="http://entropymine.com/imageworsener/slowpow/">http://entropymine.com/imageworsener/slowpow/</a></p>
<p><a href="http://stackoverflow.com/questions/9272155/replacing-extrordinarily-slow-pow-function">http://stackoverflow.com/questions/9272155/replacing-extrordinarily-slow-pow-function</a></p>
<h3 id="strings-bigstrings-and-the-ocaml-runtime-lock.">2) Strings, Bigstrings and the OCaml runtime lock.</h3>
<p>In the following code, the OCaml runtime lock is being held while the time consuming LZ4 compression routine is run. The OCaml runtime lock forces all calls into the C code to be sequential, since it behaves like a critical section.</p>
<pre class="sh_caml">CAMLprim value _LZ4_compress_bigstring(value src, value dst, value isize)
{
    CAMLparam3(src, dst, isize);
    char* src_c = get_bstr(src, 0);
    char* dst_c = get_bstr(dst, 0);

    int   res   = LZ4_compress(src_c, dst_c, Int_val(isize));
    CAMLreturn (Val_int(res));
}</pre>
<p>The code snippet can be made &quot;faster&quot; by releasing runtime lock as follows:</p>
<pre class="sh_caml">CAMLprim value _LZ4_compress_bigstring(value src, value dst, value isize)
{
    CAMLparam3(src, dst, isize);
    char* src_c = get_bstr(src, 0);
    char* dst_c = get_bstr(dst, 0);

    caml_enter_blocking_section()
    int   res   = LZ4_compress(src_c, dst_c, Int_val(isize));
    caml_leave_blocking_section()
    CAMLreturn (Val_int(res));
}</pre>
<p>Note that src and dst are Bigstrings.</p>
<p>The snippet is faster in the sense that it allows the OCaml runtime to schedule other threads that may use the runtime lock. For example, this may be the case with an Async program where many jobs are trying to compress strings and multiple such compression routines can happen on different worker threads.</p>
<p>This does not work with strings. In particular, doing the following is unsafe:</p>
<pre class="sh_caml">CAMLprim value _LZ4_compress_string(value src, value dst, value isize)
{
    CAMLparam3(src, dst, isize);
    char* src_c = String_val(src);
    char* dst_c = String_val(dst);

    caml_enter_blocking_section()
    int   res   = LZ4_compress(src_c, dst_c, Int_val(isize));
    caml_leave_blocking_section()
    CAMLreturn (Val_int(res));
}</pre>
<p>The reason is that OCaml strings live on the OCaml heap and by releasing the runtime lock, we allow the OCaml GC to run. If the OCaml GC moves the string in memory while the C code is using it, then things fail.</p>
<p>This is safe to do with Bigstrings since they are allocated on the C heap. The OCaml GC never moves them. The GC may move the stub that point to the C heap, but the values of the src_c and dst_c pointers always point to the right data.</p>
<p>Related reading: <a href="http://d.hatena.ne.jp/camlspotter/20100309/1268111257">http://d.hatena.ne.jp/camlspotter/20100309/1268111257</a></p>
<h3 id="fast-slow-and-incorrect-array-blits">3) Fast, Slow and Incorrect Array blits</h3>
<p>Array blits in OCaml, in the general case, are slow -- i.e. they are not merely memmove/memcpy calls as one might assume.</p>
<p>Array.blit as provided by Core and also supplied by OCaml call <code>caml_array_blit</code> provided by the runtime. In the OCaml runtime, there are arrays tagged as &quot;double arrays&quot; -- these are typically float arrays. Here are the rules:</p>
<ol style="list-style-type: decimal">
<li><code>caml_array_blit</code> does a memmove for double arrays. (For float arrays)</li>
<li><code>caml_array_blit</code> does a memmove for arrays in the minor heap. (i.e. for arrays created recently).</li>
<li>For every other array type there is a for-loop that calls <code>caml_modify</code>.</li>
</ol>
<p>Here, <code>caml_modify</code> is OCaml's write barrier (see notes earlier about this). Blits that fall under case 3 are an order of magnitude slower. Long lived int arrays, bool arrays and arrays of other simple immediate value representations will fall under the third case thereby incurring the cost of calling caml_modify.</p>
<h4 id="compiler-patching">1) Compiler patching</h4>
<p>It may be possible to patch the compiler to create arrays with representation guarantees at initialization time, such they are not conservatively treated as arrays of mutable references. We don't have this yet.</p>
<h4 id="use-bigarrays">2) Use Bigarrays</h4>
<p>One can use Bigarrays instead of regular OCaml arrays for immediate values. One approach to blitting Bigarrays is to use the provided blit function:</p>
<pre class="sh_caml">let blit arr ~len ~offset ~delta =
  let sub1 = Array1.sub arr offset len in
  let sub2 = Array1.sub arr (offset+delta) len in
  Array1.blit sub1 sub2</pre>
<p>However, this is not as fast as one would expect for two reasons: (1) creating sub-arrays is slow and (2) creating sub-arrays allocates words on the major heap.</p>
<p>A faster blit for Bigarrays is to (1) write your own in C OR (2) hijack the blit provided by Bigstrings (which are essentially Bigarrays). One would do the later as follows:</p>
<pre class="sh_caml">let unsafe_blit ~arr1 ~arr2 ~len ~offset1 ~offset2 ~sizeof =
  let src_pos = offset1 * sizeof in
  let dst_pos = offset2 * sizeof in
  let len = len * sizeof in
  Bigstring.unsafe_blit
    ~src:(Obj.magic arr1)
    ~dst:(Obj.magic arr2)
    ~src_pos
    ~dst_pos
    ~len</pre>
<p>This is indeed a fast blit and works between Bigarrays of immediate types such as floats, ints, bools, chars etc. Further, for large enough blits one can give up the caml runtime lock, thereby allow other ocaml code to run in parallel with the blit. (see Section on the OCaml runtime lock.)</p>
<p>To blit from a bigarray to an OCaml array one would do something like:</p>
<pre class="sh_caml">let unsafe_blit_bigstring_string ~arr1 ~arr2 ~len ~offset1 ~offset2 ~sizeof =
  let src_pos = offset1 * sizeof in
  let dst_pos = offset2 * sizeof in
  let len = len * sizeof in
  Bigstring.unsafe_blit_bigstring_string
    ~src:(Obj.magic arr1)
    ~dst:(Obj.magic arr2)
    ~src_pos
    ~dst_pos
    ~len</pre>
<p>The above snippet works well for Bigarrays of floats but not for much else, i.e. this approach does not work well if one needs to blit values our from Bigarrays to regular OCaml arrays of any type.</p>
<p>The reason this is so is that the when assigning a value into a Bigarray, the library changes the values representation from its OCaml representation to a representation compatible with native C code. (Bigarrays were after all originally designed for interop with C code). In the case of floats, OCaml does change the word level representation of floats and so the above snippet words fine. Hence the following function works as expected:</p>
<pre class="sh_caml">let snapshot_to_float_array (arr : float_bigarray) ~offset ~len =
  if len = 0 then [||]
  else begin
    let dest = Array.create ~len 0.0 in
    unsafe_blit_bigstring_string
      ~arr1:arr ~arr2:dest
      ~offset1:offset ~offset2:0
      ~len ~sizeof:sizeof_float64;
    dest
  end</pre>
<p>However for ints and other OCaml immediate types, the OCaml representation involves setting some tag bits. When values are assigned to Bigarrays, &quot;bigarr.{i} &lt;- &quot;, this representation is stripped: (x) &gt;&gt; 1. When values are read back using the getter provided, &quot;&lt;- bigarr.{i}&quot;, the tag bits are reconstructed: ((intnat)(x) &lt;&lt; 1) + 1.</p>
<p>Hence the following does not work:</p>
<pre class="sh_caml">let snapshot_to_int_array (arr : int_bigarray) ~offset ~len =
  if len = 0 then [||]
  else begin
    let dest = Array.create ~len 0 in
    unsafe_blit_bigstring_string
      ~arr1:arr ~arr2:dest
      ~offset1:offset ~offset2:0
      ~len ~sizeof:sizeof_int;
    dest
  end</pre>
<p>If one were to blit/memove the values from Bigarrays to OCaml arrays, the tag bits are not reconstructed causing numbers to be garbled and confusing the GC. The following does work however, but it is slow:</p>
<pre class="sh_caml">let snapshot_to_array bigarr ~offset ~len =
  if len = 0 then [||]
  else begin
    let arr = Array.create ~len bigarr.{0} in
    for i = 0 to len - 1 do
      arr.(i) &lt;- bigarr.{offset+i}
    done;
    arr
  end</pre>
<h4 id="use-ffi-and-build-your-own.">3) Use FFI and build your own.</h4>
<p>The most direct solution is to write your own FFI and call a memmove in C. To ensure that this is not misused for array types that may have references, the types should be fixed to specific concrete types, such as int or float.</p>
<pre class="sh_c">CAMLprim value _array_blit64(value src, value dst,
                             value src_pos, value dst_pos,
                             value len)
{
  CAMLparam5(src, dst, src_pos, dst_pos, len);
  int isrc_pos = Int_val(src_pos);
  int idst_pos = Int_val(dst_pos);
  int ilen = Int_val(len);

  int idst_len = Wosize_val(dst);
  int isrc_len = Wosize_val(src);

  // ensure we are dealing with 64 bit things
  assert(sizeof(value) == 8);

  //ensure that we are within bounds
  assert(ilen &gt;=0);
  assert(isrc_pos &gt;= 0 &amp;&amp; isrc_pos + ilen &lt;= isrc_len);
  assert(idst_pos &gt;= 0 &amp;&amp; idst_pos + ilen &lt;= idst_len);

  memmove(String_val(dst) + idst_pos * sizeof(value),
          String_val(src) + isrc_pos * sizeof(value),
          ilen * sizeof(value));

  CAMLreturn (Val_unit);
}</pre>
<p>ml/mli file bindings:</p>
<pre class="sh_caml">external int_array_blit :
  src:int array -&gt; dst:int array -&gt;
  src_pos:int -&gt; dst_pos:int -&gt;
  len:int -&gt; unit
    = &quot;_array_blit64&quot;

external float_array_blit :
  src:float array -&gt; dst:float array -&gt;
  src_pos:int -&gt; dst_pos:int -&gt;
  len:int -&gt; unit
    = &quot;_array_blit64&quot;</pre>
<p>While this gives us fast blits on immediate arrays, we do lose the ability to give up the caml runtime lock for large blits.</p>
<h3 id="time.now-and-rdtsc">4) <code>Time.now()</code> and RDTSC</h3>
<p>What maybe arguably the best approach to timing OCaml code is to manually instrument your code using Time.now. One can experiment with various configurations of the code by commenting out parts of the code to understand the relative costs of particular functions. The downside to using Time.now() is that (1) it is an expensive call and (2) it has limited resolution and is hence not useful in measuring snippets that may only take milliseconds.</p>
<p>RDTSC is a light weight CPU counter which can be used instead of Time.now(). It gives a measure of CPU cycles elapsed between subsequent calls and it useful for timing.</p>
<pre class="sh_c">CAMLprim value caml_rdtsc( )
{
    unsigned hi, lo;
    __asm__ __volatile__ (&quot;rdtsc&quot; : &quot;=a&quot;(lo), &quot;=d&quot;(hi));
    return Val_int( ((unsigned long long)lo)|( ((unsigned long long)hi)&lt;&lt;32 ));
}</pre>


      </section>
    </div>


  </body>
</html>

